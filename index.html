<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="google-site-verification" content="FeA8wZzKiI6DkLLqCJMlBhKi6PsRnR1Dmdc2TDgTYcI" />
  <title>Adrien Bardes</title>

  <link rel='icon' href='img/favicon.ico' type='image/x-icon' />
  <link href="./css/bootstrap.min.css" rel="stylesheet">
  <link rel="stylesheet" href="./assets/academicons-1.7.0/css/academicons.css" />
  <link rel="stylesheet" href="./assets/font-awesome-4.7.0/css/font-awesome.min.css" />
  <script async defer src="./js/buttons.js"></script>
  <!--   <script async defer src="https://buttons.github.io/buttons.js"></script> -->
</head>

<body>

  <!-- Navigation bar -->
  <div class="navbar navbar-default  navbar-fixed-top bg-info">
    <div class="container">
      <div class="navbar-header">

        <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
      </div>
      <div class="navbar-collapse collapse" id="navbar-main">

        <ul class="nav navbar-nav navbar-left">
          <li><a href="#home">Home</a></li>
          <li><a href="#news">News</a></li>
          <li><a href="#research">Research</a></li>
          <!-- <li><a href="#talks">Talks</a></li> -->
          <!-- <li><a href="#teaching">Teaching</a></li> -->
          <li><a href="#misc">Misc</a></li>
        </ul>
      </div>
    </div>
  </div>

  <!-- end of navigation bar -->
  <div style="height:40px;" id="home"></div>
  <div style="height:40px;"></div>

  <!-- CONTENTS -->
  <div class="container">
    <!-- Aboutme -->
    <div class="row">
      <div class="col-xs-6 col-sm-4 col-md-2">
        <a class="thumbnail">
          <img src="./img/adrien.jpg" alt="Adrien Bardes" class="img-rounded">
        </a>
      </div>

      <div class="col-xs-10 col-sm-6 col-md-4">
        <h1 class="text-info">Adrien Bardes</h1>
        <h4 class="text-info">PhD student, Meta AI & Inria</h4>
        <h5>
          <a href="mailto:adrien.bardes@inria.fr" class="text-info" title="e-Mail"><i
              class="fa fa-envelope-square fa-2x"></i></a>
          <a href="https://scholar.google.com/citations?user=SvRU8F8AAAAJ" class="text-info" title="Google Scholar"><i
              class="ai ai-google-scholar-square ai-2x"></i></a>
          <a href="https://github.com/Adrien987k" class="text-info" title="GitHub"><i
              class="fa fa-github-square fa-2x"></i></a>
          <a href="https://fr.linkedin.com/in/adrien-bardes-48a080129/" class="text-info" title="LinkedIn"><i
              class="fa fa-linkedin-square fa-2x"></i></a>
          <a href="https://twitter.com/AdrienBardes" class="text-info" title="Twitter"><i
              class="fa fa-twitter-square fa-2x"></i></a>
          <!-- <a href="https://www.youtube.com/channel/UCYJ4NEILh5DeYf-62eb1RBg" class="text-info" title="Twitter"><i
                  class="fa fa-youtube-square fa-2x"></i></a> -->
          <!-- <a href="https://who.paris.inria.fr/Clemence.Bouvier/"></a> -->
        </h5>
      </div>
    </div> <!-- end of Aboutme -->

    <p align="justify">
      <!-- I am a student researcher at <a href="https://research.google/">Google</a>, and a second-year PhD student in the
          <a href="http://www.di.ens.fr/willow">WILLOW team</a> of <a href="http://www.inria.fr/">Inria Paris</a> and <a
            href="http://www.ens.fr">&Eacute;cole Normale Sup&eacute;rieure (ENS)</a>, advised by <a
            href="https://antoine77340.github.io/">Antoine Miech</a>, <a href="http://people.ciirc.cvut.cz/~sivic/">Josef
            Sivic</a>, <a href="http://www.di.ens.fr/~laptev">Ivan Laptev</a> and <a
            href="https://www.di.ens.fr/willow/people_webpages/cordelia/">Cordelia Schmid</a>.
          My current research is focused on learning multimodal video representations using the visual and textual
          modalities.
          I received an <a
            href="https://programmes.polytechnique.edu/cycle-ingenieur-polytechnicien/cycle-ingenieur-polytechnicien">
            engineering degree </a> from <a href="https://www.polytechnique.edu/">École Polytechnique </a> and a MSc degree
          in <a href="https://www.master-mva.com/">Mathematics, Vision and Learning</a> from <a
            href="http://ens-paris-saclay.fr/">ENS Paris-Saclay</a> with highest honors and jury congratulations in 2020.
          During my engineering studies, I interned at <a href="http://www.noahlab.com.hk/">Huawei Noah's Ark Lab</a> in
          London where I worked with <a
            href="https://www.linkedin.com/in/fabio-maria-carlucci-a6662782/?originalSubdomain=uk">Fabio Maria Carlucci</a>
          on neural architecture search.
          See my <a href="https://www.linkedin.com/in/antoyang/">LinkedIn profile</a> for a full resume. -->

      I am a third-year PhD student between <a href="https://ai.facebook.com/">Meta AI</a> and the <a
        href="http://www.di.ens.fr/willow">WILLOW team</a> at <a href="http://www.inria.fr/">Inria Paris</a> and <a
        href="http://www.ens.fr">&Eacute;cole Normale Sup&eacute;rieure (ENS)</a>, advised by <a
        href="https://www.di.ens.fr/~ponce/enindex.html">Jean Ponce</a> and <a href="http://yann.lecun.com/">Yann
        LeCun</a>.

      I am interrested in the theory and practice of self-superivsed learning, with a focus
      on computer vision and learning visual representations.
    </p>
    <hr>

    <!-- News header-->
    <!-- <div class="row" id="news" style="padding-top:30px; margin-top:-60px;">
      <div class="col-md-12">
        <h2>News</h2>
      </div>
    </div> -->
    <!-- End news header -->

    <!-- FrozenBiLM accepted
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">09 / 2022</span>
          </div>
          <div class="col-sm-11 col-md-11">
            <a href="frozenbilm.html">FrozenBiLM</a> is accepted at <a href="https://nips.cc/Conferences/2022">NeurIPS
              2022</a>!
          </div>
        </div>
        <div style="height:3px;"></div> -->

    <!-- Google internship -->
    <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">06 / 2022</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I am starting a 6-month research internship at <a href="https://research.google/">Google Research</a> in
            Grenoble.
          </div>
        </div>
        <div style="height:3px;"></div> -->

    <!-- Just Ask extension accepted -->
    <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">04 / 2022</span>
          </div>
          <div class="col-sm-11 col-md-11">
            <a href="just-ask.html">Just Ask extension</a> is accepted at the <a
              href="https://www.computer.org/csdl/journal/tp">TPAMI Special Issue on the Best Papers of ICCV 2021</a>!
          </div>
        </div>
        <div style="height:3px;"></div> -->

    <!-- TubeDETR accepted https://flatui.colorion.co/palette/87 -->
    <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">03 / 2022</span>
          </div>
          <div class="col-sm-11 col-md-11">
            <a href="tubedetr.html">TubeDETR</a> is accepted at <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a> as an
            oral!
          </div>
        </div>
        <div style="height:3px;"></div> -->

    <!-- Just Ask accepted -->
    <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">07 / 2021</span>
          </div>
          <div class="col-sm-11 col-md-11">
            <a href="just-ask.html">Just Ask</a> is accepted at <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a> as
            an oral!
          </div>
        </div>
        <div style="height:3px;"></div> -->

    <!-- Just Ask - HVU - CVPR21 Talk -->
    <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">06 / 2021</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I gave talks about <a href="just-ask.html">Just Ask</a> at the <a
              href="https://holistic-video-understanding.github.io/workshops/cvpr2021.html"> CVPR 2021 Holistic Video
              Understanding Workshop</a> and at the <a href="https://www.rocq.inria.fr/semdoc/">Inria Junior Seminar</a>.
          </div>
        </div>
        <div style="height:3px;"></div> -->

    <!-- Willow PhD -->
    <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">09 / 2020</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I am starting my PhD at <a href="http://www.di.ens.fr/willow">Inria WILLOW</a>.
          </div>
        </div>
        <div style="height:3px;"></div> -->

    <!-- MVA degree -->
    <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">09 / 2020</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I have received my <a href="https://www.master-mva.com/"> MSc degree </a> with highest honors and jury
            congratulations from <a href="https://ens-paris-saclay.fr/">ENS Paris-Saclay</a>.
          </div>
        </div>
        <div style="height:3px;"></div> -->

    <!-- Willow internship -->
    <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">04 / 2020</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I am starting a 5-month research internship at <a href="http://www.di.ens.fr/willow">Inria WILLOW</a> in Paris.
          </div>
        </div>
        <div style="height:3px;"></div> -->

    <!-- NASEFH accepted -->
    <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">12 &nbsp;/ 2019 </span>
          </div>
          <div class="col-sm-11 col-md-11">
            <a href="https://iclr.cc/virtual_2020/poster_HygrdpVKvr.html">NAS evaluation is frustratingly hard</a> is
            accepted at <a href="https://iclr.cc/Conferences/2020">ICLR 2020</a>!
          </div>
        </div>
        <div style="height:3px;"></div> -->

    <!-- X degree -->
    <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">09 / 2019</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I have received my <a
              href="https://programmes.polytechnique.edu/cycle-ingenieur-polytechnicien/cycle-ingenieur-polytechnicien">
              engineering degree </a> from <a href="https://www.polytechnique.edu/">École Polytechnique</a>.
          </div>
        </div>
        <div style="height:3px;"></div> -->

    <!-- Carnegie Mellor internship -->
    <!-- <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">04 / 2019</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I am starting a 5-month research internship at the <a href="">Robotics Institute</a> of  in
            London.
          </div>
        </div>
        <div style="height:3px;"></div> -->

    <!-- end of news -->
    <!-- <hr> -->

    <!-- Research -->
    <div class="row" id="research" style="padding-top:30px; margin-top:-60px;">
      <div class="col-md-12">
        <h2>Research</h2>

        <!-- VICRegL, NeurIPS 2022 -->
        <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="./img/vicregl.png" alt="VICRegL: Self-Supervised Learning of Local Visual Features">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>VICRegL: Self-Supervised Learning of Local Visual Features</strong> </br>
            <b>Adrien Bardes</b>, Jean Ponce, and Yann LeCun<br>
            NeurIPS 2022 <br>
            <a href="https://arxiv.org/abs/2210.01571"><button type="button"
                class="btn btn-primary btn-xs">arxiv</button></a>
            <a href="https://arxiv.org/pdf/2210.01571.pdf"><button type="button"
                class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex6">bibtex</button>
            <div id="bibtex6" class="collapse">
              <pre><tt>@inproceedings{bardes2022vicregl,
    title = {VICRegL: Self-Supervised Learning of Local Visual Features},
    author = {Adrien Bardes and Jean Ponce and Yann LeCun},
    booktitle={arXiv preprint arXiv:2206.08155}
    year = {2022},
    }</tt></pre>
            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract6">abstract</button>
            <div id="abstract6" class="collapse">
              <p style="text-align: justify;">
                Most recent self-supervised methods for learning image representations focus on
                either producing a global feature with invariance properties, or producing a set of
                local features. The former works best for classification tasks while the latter is best
                for detection and segmentation tasks. This paper explores the fundamental trade-
                off between learning local and global features. A new method called VICRegL
                is proposed that learns good global and local features simultaneously, yielding
                excellent performance on detection and segmentation tasks while maintaining
                good performance on classification tasks. Concretely, two identical branches of a
                standard convolutional net architecture are fed two differently distorted versions
                of the same image. The VICReg criterion is applied to pairs of global feature
                vectors. Simultaneously, the VICReg criterion is applied to pairs of local feature
                vectors occurring before the last pooling layer. Two local feature vectors are
                attracted to each other if their l2-distance is below a threshold or if their relative
                locations are consistent with a known geometric transformation between the two
                input images. We demonstrate strong performance on linear classification and
                segmentation transfer tasks. Code and pretrained models are publicly available at:
                https://github.com/facebookresearch/VICRegL
              </p>
            </div>
            <a href="https://github.com/facebookresearch/VICRegL"><button type="button"
                class="btn btn-primary btn-xs">code</button></a>
          </div>
        </div>
        <!-- end of VICRegL -->

        <!-- Guillotine, arxiv 2022 -->
        <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="./img/guillotine.png"
                alt="Guillotine Regularization: Improving Deep Networks Generalization by Removing their Head">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>Guillotine Regularization: Improving Deep Networks Generalization by Removing their Head</strong>
            </br>
            Florian Bordes, Randall Balestriero, Quentin Garrido, <b>Adrien Bardes</b> and Pascal Vincent<br>
            arXiv 2022 <br>
            <a href="https://arxiv.org/abs/2206.13378"><button type="button"
                class="btn btn-primary btn-xs">arxiv</button></a>
            <a href="https://arxiv.org/pdf/2206.13378.pdf"><button type="button"
                class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex6">bibtex</button>
            <div id="bibtex6" class="collapse">
              <pre><tt>@article{bordes2022guillotine,
    title = {Guillotine Regularization: Improving Deep Networks Generalization by Removing their Head},
    author = {Florian Bordes and Randall Balestriero and Quentin Garrido and Adrien Bardes and Pascal Vincent},
    booktitle={arXiv preprint arXiv:2206.13378}
    year = {2022},
    }</tt></pre>
            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract6">abstract</button>
            <div id="abstract6" class="collapse">
              <p style="text-align: justify;">
                One unexpected technique that emerged in recent years consists in training a Deep Network (DN) with a
                Self-Supervised Learning (SSL) method, and using this network on downstream tasks but with its last few
                layers entirely removed. This usually skimmed-over trick is actually critical for SSL methods to display
                competitive performances. For example, on ImageNet classification, more than 30 points of percentage can
                be gained that way. This is a little vexing, as one would hope that the network layer at which
                invariance is explicitly enforced by the SSL criterion during training (the last layer) should be the
                one to use for best generalization performance downstream. But it seems not to be, and this study sheds
                some light on why. This trick, which we name Guillotine Regularization (GR), is in fact a generically
                applicable form of regularization that has also been used to improve generalization performance in
                transfer learning scenarios. In this work, through theory and experiments, we formalize GR and identify
                the underlying reasons behind its success in SSL methods. Our study shows that the use of this trick is
                essential to SSL performance for two main reasons: (i) improper data-augmentations to define the
                positive pairs used during training, and/or (ii) suboptimal selection of the hyper-parameters of the SSL
                loss.
              </p>
            </div>
            <!-- <a href="https://github.com/facebookresearch/VICRegL"><button type="button"
                class="btn btn-primary btn-xs">code</button></a> -->
          </div>
        </div>
        <!-- end of Guillotine -->


        <!-- Intra-Instance, arxiv 2022 -->
        <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="./img/intra.png" alt="Intra-Instance VICReg: Bag of Self-Supervised Image Patch Embedding
              ">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>Intra-Instance VICReg: Bag of Self-Supervised Image Patch Embedding
            </strong> </br>
            Yubei Chen*, <b>Adrien Bardes</b>*, Zengyi Li and Yann LeCun<br>
            arvix 2022 <br>
            <a href="https://arxiv.org/abs/2206.08954"><button type="button"
                class="btn btn-primary btn-xs">arxiv</button></a>
            <a href="https://arxiv.org/pdf/2206.08954.pdf"><button type="button"
                class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex6">bibtex</button>
            <div id="bibtex6" class="collapse">
              <pre><tt>@article{chen2022intra,
    title = {Intra-Instance VICReg: Bag of Self-Supervised Image Patch Embedding},
    author = {Yubei Chen and Adrien Bardes and Zengyi Li and Yann LeCun},
    journal={arXiv preprint arXiv:2206.08954}
    year = {2022},
    }</tt></pre>
            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract6">abstract</button>
            <div id="abstract6" class="collapse">
              <p style="text-align: justify;">
                Recently, self-supervised learning (SSL) has achieved tremendous empirical advancements in learning
                image representation. However, our understanding and knowledge of the representation are still limited.
                This work shows that the success of the SOTA siamese-network-based SSL approaches is primarily based on
                learning a representation of image patches. Particularly, we show that when we learn a representation
                only for fixed-scale image patches and aggregate different patch representations linearly for an image
                (instance), it can achieve on par or even better results than the baseline methods on several
                benchmarks. Further, we show that the patch representation aggregation can also improve various SOTA
                baseline methods by a large margin. We also establish a formal connection between the SSL objective and
                the image patches co-occurrence statistics modeling, which supplements the prevailing invariance
                perspective. By visualizing the nearest neighbors of different image patches in the embedding space and
                projection space, we show that while the projection has more invariance, the embedding space tends to
                preserve more equivariance and locality. Finally, we propose a hypothesis for the future direction based
                on the discovery of this work.
              </p>
            </div>
            <!-- <a href="https://github.com/facebookresearch/VICRegL"><button type="button"
                class="btn btn-primary btn-xs">code</button></a> -->
          </div>
        </div>
        <!-- end of Intra-Instance -->

        <!-- Duality, arxiv 2022 -->
        <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="./img/duality.png"
                alt="On the duality between contrastive and non-contrastive self-supervised learning">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>On the duality between contrastive and non-contrastive self-supervised learning
            </strong> </br>
            Quentin Garrido, Yubei Chen, <u>Adrien Bardes</u>, Laurent Najman and Yann Lecun<br>
            arvix 2022 <br>
            <a href="https://arxiv.org/abs/2206.02574"><button type="button"
                class="btn btn-primary btn-xs">arxiv</button></a>
            <a href="https://arxiv.org/pdf/2206.02574.pdf"><button type="button"
                class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex6">bibtex</button>
            <div id="bibtex6" class="collapse">
              <pre><tt>@article{garrido2022duality,
    title = {On the duality between contrastive and non-contrastive self-supervised learning},
    author = {Quentin Garrido and Yubei Chen and Adrien Bardes and Laurent Najman and Yann Lecun},
    journal={arXiv preprint arXiv:2206.02574}
    year = {2022},
    }</tt></pre>
            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract6">abstract</button>
            <div id="abstract6" class="collapse">
              <p style="text-align: justify;">
                Recent approaches in self-supervised learning of image representations can be categorized into different
                families of methods and, in particular, can be divided into contrastive and non-contrastive approaches.
                While differences between the two families have been thoroughly discussed to motivate new approaches, we
                focus more on the theoretical similarities between them. By designing contrastive and covariance based
                non-contrastive criteria that can be related algebraically and shown to be equivalent under limited
                assumptions, we show how close those families can be. We further study popular methods and introduce
                variations of them, allowing us to relate this theoretical result to current practices and show the
                influence (or lack thereof) of design choices on downstream performance. Motivated by our equivalence
                result, we investigate the low performance of SimCLR and show how it can match VICReg's with careful
                hyperparameter tuning, improving significantly over known baselines. We also challenge the popular
                assumptions that contrastive and non-contrastive methods, respectively, need large batch sizes and
                output dimensions. Our theoretical and quantitative results suggest that the numerical gaps between
                contrastive and non-contrastive methods in certain regimes can be closed given better network design
                choices and hyperparameter tuning. The evidence shows that unifying different SOTA methods is an
                important direction to build a better understanding of self-supervised learning.
              </p>
            </div>
            <!-- <a href="https://github.com/facebookresearch/VICRegL"><button type="button"
                class="btn btn-primary btn-xs">code</button></a> -->
          </div>
        </div>
        <!-- end of Duality -->


        <!-- VICReg, ICLR 2022 -->
        <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="./img/vicreg.jpg"
                alt="VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning
            </strong> </br>
            <u>Adrien Bardes</u>, Jean Ponce and Yann Lecun<br>
            ICLR 2022 <br>
            <a href="https://arxiv.org/abs/2105.04906"><button type="button"
                class="btn btn-primary btn-xs">arxiv</button></a>
            <a href="https://arxiv.org/pdf/2105.04906.pdf"><button type="button"
                class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex6">bibtex</button>
            <div id="bibtex6" class="collapse">
              <pre><tt>@inproceedings{bardes2022vicreg,
    title = {VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning},
    author = {Adrien Bardes and Jean Ponce and Yann Lecun},
    booktitle = {ICLR 2022}
    year = {2022},
    }</tt></pre>
            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract6">abstract</button>
            <div id="abstract6" class="collapse">
              <p style="text-align: justify;">
                Recent self-supervised methods for image representation learning are based on maximizing the agreement
                between embedding vectors from different views of the same image. A trivial solution is obtained when
                the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in
                the learning architecture, that often lack a clear justification or interpretation. In this paper, we
                introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the
                collapse problem with a simple regularization term on the variance of the embeddings along each
                dimension individually. VICReg combines the variance term with a decorrelation mechanism based on
                redundancy reduction and covariance regularization, and achieves results on par with the state of the
                art on several downstream tasks. In addition, we show that incorporating our new variance term into
                other methods helps stabilize the training and leads to performance improvements.
              </p>
            </div>
            <a href="https://github.com/facebookresearch/vicreg"><button type="button"
                class="btn btn-primary btn-xs">code</button></a>
          </div>
        </div>
        <!-- end of VICReg -->


        <!-- Hallucinates, ICCV 2022 -->
        <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="./img/dmas.png" alt="Learning To Hallucinate Examples From Extrinsic and Intrinsic Supervision">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>Learning To Hallucinate Examples From Extrinsic and Intrinsic Supervision
            </strong> </br>
            Liangke Gui*, <u>Adrien Bardes</u>*, Ruslan Salakhutdinov, Alexander Hauptmann, Martial Hebert and Yu-Xiong
            Wang<br>
            ICCV 2022 <br>
            <!-- <a href="https://arxiv.org/abs/2105.04906"><button type="button"
                class="btn btn-primary btn-xs">arxiv</button></a> -->
            <a
              href="https://openaccess.thecvf.com/content/ICCV2021/papers/Gui_Learning_To_Hallucinate_Examples_From_Extrinsic_and_Intrinsic_Supervision_ICCV_2021_paper.pdf"><button
                type="button" class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex6">bibtex</button>
            <div id="bibtex6" class="collapse">
              <pre><tt>@inproceedings{gui2021dmas,
    title = {Learning To Hallucinate Examples From Extrinsic and Intrinsic Supervision},
    author = {Liangke Gui and Adrien Bardes and Ruslan Salakhutdinov and Alexander Hauptmann and Martial Hebert and Yu-Xiong},
    booktitle = {ICCV 2021}
    year = {2021},
    }</tt></pre>
            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract6">abstract</button>
            <div id="abstract6" class="collapse">
              <p style="text-align: justify;">
                Learning to hallucinate additional examples has recently been shown as a promising direction to address
                few-shot learning tasks. This work investigates two important yet overlooked natural supervision signals
                for guiding the hallucination process--(i) extrinsic: classifiers trained on hallucinated examples
                should be close to strong classifiers that would be learned from a large amount of real examples; and
                (ii) intrinsic: clusters of hallucinated and real examples belonging to the same class should be pulled
                together, while simultaneously pushing apart clusters of hallucinated and real examples from different
                classes. We achieve (i) by introducing an additional mentor model on data-abundant base classes for
                directing the hallucinator, and achieve (ii) by performing contrastive learning between hallucinated and
                real examples. As a general, model-agnostic framework, our dual mentor-and self-directed (DMAS)
                hallucinator significantly improves few-shot learning performance on widely used benchmarks in various
                scenarios.
              </p>
            </div>
            <!-- <a href="https://github.com/facebookresearch/vicreg"><button type="button"
                class="btn btn-primary btn-xs">code</button></a> -->
          </div>
        </div>
        <!-- end of Hallucinate -->

      </div>
    </div> <!-- end of projects -->

    <hr>

    <!-- Talks -->
    <!-- <div class="row" id="talks" style="padding-top:30px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>Talks</h2>
            <ul>
              <li>06/2022 - <a href="https://cvpr2022.thecvf.com/">CVPR 2022 Oral Session (New Orleans, Louisiana) </a> - <a
                  href="https://www.youtube.com/watch?v=FT_50ylQMNs&ab_channel=AntoineYang">[5 min Video] </a> - <a
                  href="slides/tubedetr-cvpr.pdf"> [Slides] </a> - <a href="slides/tubedetr-cvpr-poster.pdf"> [Poster] </a>
              </li>
              <li>06/2022 - <a href="https://www.di.ens.fr/"> Seminar of the Computer Science department of &Eacute;cole
                  Normale Sup&eacute;rieure (Mûr-de-Bretagne, France) </a> - <a href="slides/frozenbilm-diens.pdf"> [Slides]
                </a> </li>
              <li>10/2021 - <a href="https://iccv2021.thecvf.com/home">ICCV 2021 Oral Session (Virtual) </a> - <a
                  href="https://www.youtube.com/watch?v=eyEW5Z23Ofs&ab_channel=AntoineYang">[12 min Video] </a> - <a
                  href="slides/just-ask-iccv.pdf"> [Slides] </a> - <a href="slides/just-ask-iccv-poster.pdf"> [Poster] </a>
              </li>
              <li>06/2021 - <a href="https://holistic-video-understanding.github.io/workshops/cvpr2021.html">CVPR 2021
                  Holistic Video Understanding Workshop (Virtual) </a> - <a href="slides/just-ask-hvu-cvpr21.pdf"> [Slides]
                </a> - <a href="https://youtu.be/jzXdRT5W3C4?t=17280"> [Recording] </a> </li>
              <li>05/2021 - <a href="https://www.rocq.inria.fr/semdoc/">Inria Junior Seminar (Virtual) </a> - <a
                  href="https://www.rocq.inria.fr/semdoc/Presentations/just-ask-junior-seminar.pdf"> [Slides] </a> - <a
                  href="https://webconf.math.cnrs.fr/playback/presentation/2.0/playback.html?meetingId=38b91f46f61e82195e2275670f5d26218e2f9bae-1621340178022">
                  [Recording] </a> </li>
              <li>04/2020 - <a href="https://iclr.cc/virtual_2020/poster_HygrdpVKvr.html">ICLR 2020 Poster Session (Virtual)
                </a> - <a href="https://www.youtube.com/watch?v=jB7vrGR_4ak&ab_channel=AntoineYang">[5 min Video] </a> - <a
                  href="slides/nasefh-iclr20.pdf"> [Slides] </a></li>
            </ul>
          </div>
        </div> -->

    <!-- <hr> -->

    <!-- Teaching -->
    <!-- <div class="row" id="teaching" style="padding-top:30px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>Teaching</h2> -->

    <!-- ORCV 2022 -->
    <!-- <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Fall&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2022</span>
              </div>
              <div class="col-sm-11 col-md-11">
                &nbsp; Object Recognition and Computer Vision, Teacher Assistant - Master level (MVA) - 50 hours - ENS
                Paris-Saclay
              </div>
            </div> -->

    <!-- </div> -->
    <!-- </div> end of teaching -->

    <!-- <hr> -->

    <!-- Misc -->
    <div class="row" id="misc" style="padding-top:30px; margin-top:-60px;">
      <div class="col-md-12">
        <h2>Misc.</h2>
        Reviewer for ICLR, NeurIPS, IJCV and Springer ML.
      </div>
    </div> <!-- end of misc -->

    <hr>

    <div class="container">
      <footer>
        <p align="right"><small>Copyright © Adrien Bardes &nbsp;/&nbsp; Last update: Octobre 2022 </small></p>
      </footer>
      <div style="height:10px;"></div>
    </div>

    <!-- Bootstrap core JavaScript -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="jq/jquery-1.11.1.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/docs.min.js"></script>

    <script>
      (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
          (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
          m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
      })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

      ga('create', 'UA-75922772-1', 'auto');
      ga('send', 'pageview');

    </script>
  </div>
</body>

</html>