<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="google-site-verification" content="FeA8wZzKiI6DkLLqCJMlBhKi6PsRnR1Dmdc2TDgTYcI" />
  <title>Adrien Bardes</title>

  <link rel='icon' href='img/favicon.ico' type='image/x-icon' />
  <link href="./css/bootstrap.min.css" rel="stylesheet">
  <link rel="stylesheet" href="./assets/academicons-1.7.0/css/academicons.css" />
  <link rel="stylesheet" href="./assets/font-awesome-4.7.0/css/font-awesome.min.css" />
  <!-- <script async defer src="./js/buttons.js"></script> -->
  <script async defer src="https://buttons.github.io/buttons.js"></script>
</head>

<body>

  <!-- Navigation bar -->
  <div class="navbar navbar-default  navbar-fixed-top bg-info">
    <div class="container">
      <div class="navbar-header">

        <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
      </div>
      <div class="navbar-collapse collapse" id="navbar-main">

        <ul class="nav navbar-nav navbar-left">
          <li><a href="#home">Home</a></li>
          <!-- <li><a href="#news">News</a></li> -->
          <li><a href="#research">Research</a></li>
          <li><a href="#talks">Talks</a></li>
          <li><a href="#teaching">Teaching</a></li>
          <li><a href="#misc">Academic Services</a></li>
        </ul>
      </div>
    </div>
  </div>

  <!-- end of navigation bar -->
  <div style="height:40px;" id="home"></div>
  <div style="height:40px;"></div>

  <!-- CONTENTS -->
  <div class="container">
    <!-- Aboutme -->
    <div class="row">
      <div class="col-xs-6 col-sm-4 col-md-2">
        <a class="thumbnail">
          <img src="./img/adrien.jpg" alt="Adrien Bardes" class="img-rounded">
        </a>
      </div>

      <div class="col-xs-10 col-sm-6 col-md-4">
        <h1 class="text-info">Adrien Bardes</h1>
        <h4 class="text-info">PhD student, Meta AI & Inria</h4>
        <h5>
          <a href="mailto:adrien.bardes@inria.fr" class="text-info" title="e-Mail" target="_blank"><i
              class=" fa fa-envelope-square fa-2x"></i></a>
          <a href="https://scholar.google.com/citations?user=SvRU8F8AAAAJ" class="text-info" title="Google Scholar"
            target="_blank"><i class=" ai ai-google-scholar-square ai-2x"></i></a>
          <a href="https://github.com/Adrien987k" class="text-info" title="GitHub" target="_blank"><i
              class=" fa fa-github-square fa-2x"></i></a>
          <a href="https://fr.linkedin.com/in/adrien-bardes-48a080129/" class="text-info" title="LinkedIn"
            target="_blank"><i class=" fa fa-linkedin-square fa-2x"></i></a>
          <a href="https://twitter.com/AdrienBardes" class="text-info" title="Twitter" target="_blank"><i
              class=" fa fa-twitter-square fa-2x"></i></a>
          <a href="https://scholar.google.com/citations?user=SvRU8F8AAAAJ&hl=en" class="text-info" title="CV"
            target="_blank"><i class=" fa fa-twitter-square fa-2x"></i></a>
        </h5>
      </div>
    </div> <!-- end of Aboutme -->

    <p align="justify">
      I am a third-year PhD student (graduating in Spring'2024) between <a href="https://ai.facebook.com/">Meta AI</a>
      and
      the <a href="http://www.di.ens.fr/willow">WILLOW team</a> at <a href="http://www.inria.fr/">Inria Paris</a> and <a
        href="http://www.ens.fr">&Eacute;cole Normale Sup&eacute;rieure (ENS)</a>, advised by <a
        href="https://www.di.ens.fr/~ponce/enindex.html">Jean Ponce</a> and <a href="http://yann.lecun.com/">Yann
        LeCun</a>.
      My research interests lie between theory and practice of self-supervised learning from visual inputs.
      I strongly believe that vision is a core component of human intelligence and that future AI systems
      will understand the world by self-learning from visual data such as images and videos.

      <br><br>

      I have received a MSc degree
      in <a href="https://www.master-mva.com/">Mathematics, Vision and Learning</a>, a BSc in theoritical computer
      science from <a href="http://ens-paris-saclay.fr/">ENS Paris-Saclay</a>, and a BSc in mathematics and computer
      science from <a href="https://www.u-pec.fr/">University Paris-Est Creteil</a>. I have interned at <a
        href="https://www.cmu.edu/">Carnegie Mellon University</a>
      in the <a href="https://www.ri.cmu.edu/">Robotics Institute</a> working with <a
        href="https://www.cs.cmu.edu/about-scs/about-dean">Martiel Hebert</a> and <a
        href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a> on few-shot learning in computer vision.
    </p>
    <hr>

    <!-- Research -->
    <div class="row" id="research" style="padding-top:30px; margin-top:-60px;">
      <div class="col-md-12">
        <h2>Research</h2>


        <!-- TODO V-JEPA -->
        <!-- <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="TODO IMG PATH" alt="TODO TITLE">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>TODO TITLE</strong> </br>
            TODO AUTHORS <br>TODO CONF<br>
            <a target="_blank" href="TODO"> <button type="button" class="btn btn-primary btn-xs"> arxiv </button></a>
            <a target="_blank" href="TODO"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex7">bibtex</button>
            <div id="bibtex7" class="collapse">
              <pre><tt>@inproceedings{TODO,
            title = {TODO},
            author = {TODO},
            booktitle={arXiv preprint arXiv:2206.08155}
            year = {2022},
            }</tt></pre>

            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract7">abstract</button>
            <div id="abstract7" class="collapse">
              <p style="text-align: justify;">
                TODO
              </p>
            </div>
            <a target="_blank" href="TODO"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          </div>
        </div> -->
        <!-- end of TODO -->

        <!-- TODO BoB -->
        <!-- <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="TODO IMG PATH" alt="TODO TITLE">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>TODO TITLE</strong> </br>
            TODO AUTHORS <br>TODO CONF<br>
            <a target="_blank" href="TODO"> <button type="button" class="btn btn-primary btn-xs"> arxiv </button></a>
            <a target="_blank" href="TODO"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex7">bibtex</button>
            <div id="bibtex7" class="collapse">
              <pre><tt>@inproceedings{TODO,
            title = {TODO},
            author = {TODO},
            booktitle={arXiv preprint arXiv:2206.08155}
            year = {2022},
            }</tt></pre>

            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract7">abstract</button>
            <div id="abstract7" class="collapse">
              <p style="text-align: justify;">
                TODO
              </p>
            </div>
            <a target="_blank" href="TODO"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          </div>
        </div> -->
        <!-- end of TODO -->

        <!-- MC-JEPA -->
        <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="./img/mcjepa.png" width="1738" height="763"
                alt="MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and
              Content Features</strong> </br>
            <u>Adrien Bardes</u>, Jean Ponce, Yann LeCun<br>arxiv 2023<br>
            <a target="_blank" href="https://arxiv.org/abs/2307.12698"> <button type="button"
                class="btn btn-primary btn-xs"> arxiv </button></a>
            <a target="_blank" href="https://arxiv.org/pdf/2307.12698.pdf"><button type="button"
                class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex7">bibtex</button>
            <div id="bibtex7" class="collapse">
              <pre><tt>@inproceedings{bardes2023mcjepa,
            title = {MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features},
            author = {Adrien Bardes and Jean Ponce and Yann LeCun},
            booktitle={arXiv preprint arXiv:2307.12698}
            year = {2023},
            }</tt></pre>

            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract7">abstract</button>
            <div id="abstract7" class="collapse">
              <p style="text-align: justify;">
                Self-supervised learning of visual representations has been focusing on learning content features, which
                do not capture object motion or location, and focus on identifying and differentiating objects in images
                and videos. On the other hand, optical flow estimation is a task that does not involve understanding the
                content of the images on which it is estimated. We unify the two approaches and introduce MC-JEPA, a
                joint-embedding predictive architecture and self-supervised learning approach to jointly learn optical
                flow and content features within a shared encoder, demonstrating that the two associated objectives; the
                optical flow estimation objective and the self-supervised learning objective; benefit from each other
                and thus learn content features that incorporate motion information. The proposed approach achieves
                performance on-par with existing unsupervised optical flow benchmarks, as well as with common
                self-supervised learning approaches on downstream tasks such as semantic segmentation of images and
                videos.
              </p>
            </div>
            <!-- <a target="_blank" href="TODO"><button type="button" class="btn btn-primary btn-xs">code</button></a> -->
          </div>
        </div>
        <!-- end of MC-JEPA -->

        <!-- Cookbook -->
        <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="./img/cookbook.png" width="1738" height="763" alt="A Cookbook of Self-Supervised Learning">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>A Cookbook of Self-Supervised Learning</strong> </br>
            Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein, Florian Bordes,
            Adrien Bardes, Gregoire Mialon, Yuandong Tian, Avi Schwarzschild, Andrew Gordon Wilson, Jonas Geiping,
            Quentin Garrido, Pierre Fernandez, Amir Bar, Hamed Pirsiavash, Yann LeCun, Micah Goldblum <br>ICML Tutorials
            2023<br>
            <a target="_blank" href="https://arxiv.org/abs/2304.12210"> <button type="button"
                class="btn btn-primary btn-xs"> arxiv </button></a>
            <a target="_blank" href="https://arxiv.org/pdf/2304.12210.pdf"><button type="button"
                class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex7">bibtex</button>
            <div id="bibtex7" class="collapse">
              <pre><tt>@inproceedings{balestriero2023cokbook,
            title = {A Cookbook of Self-Supervised Learning},
            author = {Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein, Florian Bordes,
              Adrien Bardes, Gregoire Mialon, Yuandong Tian, Avi Schwarzschild, Andrew Gordon Wilson, Jonas Geiping,
              Quentin Garrido, Pierre Fernandez, Amir Bar, Hamed Pirsiavash, Yann LeCun, Micah Goldblum},
            booktitle={arXiv preprint arXiv:2304.12210}
            year = {2023},
            }</tt></pre>

            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract7">abstract</button>
            <div id="abstract7" class="collapse">
              <p style="text-align: justify;">
                Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine
                learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry.
                While many components are familiar, successfully training a SSL method involves a dizzying set of
                choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry
                into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope
                to empower the curious researcher to navigate the terrain of methods, understand the role of the various
                knobs, and gain the know-how required to explore how delicious SSL can be.
              </p>
            </div>
            <a target="_blank" href="TODO"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          </div>
        </div>
        <!-- end of Cookbook -->

        <!-- NFL-SSRL -->
        <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="./img/nfl.png" width="1738" height="763"
                alt="No Free Lunch in Self-Supervised Representation Learning">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>No Free Lunch in Self-Supervised Representation Learning</strong> </br>
            Ihab Bendidi, <u>Adrien Bardes</u>, Ethan Cohen, Alexis Lamiable, Guillaume Bollot, Auguste Gen-
            ovesio <br>arvix 2023<br>
            <a target="_blank" href="https://arxiv.org/abs/2304.11718"> <button type="button"
                class="btn btn-primary btn-xs"> arxiv </button></a>
            <a target="_blank" href="https://arxiv.org/pdf/2304.11718.pdf"><button type="button"
                class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex7">bibtex</button>
            <div id="bibtex7" class="collapse">
              <pre><tt>@article{bendidi2023nfl,
            title = {No Free Lunch in Self-Supervised Representation Learning},
            author = {Ihab Bendidi and Adrien Bardes and Ethan Cohen and Alexis Lamiable and Guillaume Bollot and Auguste Genovesio},
            booktitle={arXiv preprint arXiv:2304.11718}
            year = {2023},
            }</tt></pre>
            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract7">abstract</button>
            <div id="abstract7" class="collapse">
              <p style="text-align: justify;">
                Self-supervised representation learning in computer vision relies heavily on hand-crafted image
                transformations to learn meaningful and invariant features. However few extensive explorations of
                the
                impact of transformation design have been conducted in the literature. In particular, the dependence
                of
                downstream performances to transformation design has been established, but not studied in depth. In
                this
                work, we explore this relationship, its impact on a domain other than natural images, and show that
                designing the transformations can be viewed as a form of supervision. First, we demonstrate that not
                only do transformations have an effect on downstream performance and relevance of clustering, but
                also
                that each category in a supervised dataset can be impacted in a different way. Following this, we
                explore the impact of transformation design on microscopy images, a domain where the difference
                between
                classes is more subtle and fuzzy than in natural images. In this case, we observe a greater impact
                on
                downstream tasks performances. Finally, we demonstrate that transformation design can be leveraged
                as a
                form of supervision, as careful selection of these by a domain expert can lead to a drastic increase
                in
                performance on a given downstream task.
              </p>
            </div>
            <!-- <a target="_blank" href="TODO"><button type="button" class="btn btn-primary btn-xs">code</button></a> -->
          </div>
        </div>
        <!-- end of NFL -->

        <!-- Guillotine -->
        <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="./img/guillotine.png" width="1738" height="763"
                alt="Guillotine Regularization: Improving Deep Networks Generalization by Removing their Head">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>Guillotine Regularization: Improving Deep Networks Generalization by Removing their
              Head</strong>
            </br>
            Florian Bordes, Randall Balestriero, Quentin Garrido, <u>Adrien Bardes</u> and Pascal
            Vincent<br>TMLR,
            2023<br>
            <a href="https://arxiv.org/abs/2206.13378"><button type="button"
                class="btn btn-primary btn-xs">arxiv</button></a>
            <a href="https://arxiv.org/pdf/2206.13378.pdf"><button type="button"
                class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex5">bibtex</button>
            <div id="bibtex5" class="collapse">
              <pre><tt>@article{bordes2023guillotine,
    title = {Guillotine Regularization: Why Removing Layers is Needed to Improve Generalization in Self-Supervised Learning},
    author = {Florian Bordes and Randall Balestriero and Quentin Garrido and Adrien Bardes and Pascal Vincent},
    journal={TMLR}
    year = {2023},
    }</tt></pre>
            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract5">abstract</button>
            <div id="abstract5" class="collapse">
              <p style="text-align: justify;">
                One unexpected technique that emerged in recent years consists in training a Deep Network (DN)
                with
                a
                Self-Supervised Learning (SSL) method, and using this network on downstream tasks but with its
                last
                few
                layers entirely removed. This usually skimmed-over trick is actually critical for SSL methods to
                display
                competitive performances. For example, on ImageNet classification, more than 30 points of
                percentage
                can
                be gained that way. This is a little vexing, as one would hope that the network layer at which
                invariance is explicitly enforced by the SSL criterion during training (the last layer) should
                be
                the
                one to use for best generalization performance downstream. But it seems not to be, and this
                study
                sheds
                some light on why. This trick, which we name Guillotine Regularization (GR), is in fact a
                generically
                applicable form of regularization that has also been used to improve generalization performance
                in
                transfer learning scenarios. In this work, through theory and experiments, we formalize GR and
                identify
                the underlying reasons behind its success in SSL methods. Our study shows that the use of this
                trick
                is
                essential to SSL performance for two main reasons: (i) improper data-augmentations to define the
                positive pairs used during training, and/or (ii) suboptimal selection of the hyper-parameters of
                the
                SSL
                loss.
              </p>
            </div>
            <!-- <a href="https://github.com/facebookresearch/VICRegL"><button type="button"
                class="btn btn-primary btn-xs">code</button></a> -->
          </div>
        </div>
        <!-- end of Guillotine -->


        <!-- Intra-Instance, arxiv 2022 -->
        <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="./img/intra.png" width="1738" height="763" alt="Intra-Instance VICReg: Bag of Self-Supervised Image Patch Embedding
              ">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>Intra-Instance VICReg: Bag of Self-Supervised Image Patch Embedding
            </strong> </br>
            Yubei Chen*, <u>Adrien Bardes</u>*, Zengyi Li and Yann LeCun<br>
            arvix 2022 <br>
            <a href="https://arxiv.org/abs/2206.08954"><button type="button"
                class="btn btn-primary btn-xs">arxiv</button></a>
            <a href="https://arxiv.org/pdf/2206.08954.pdf"><button type="button"
                class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex4">bibtex</button>
            <div id="bibtex4" class="collapse">
              <pre><tt>@article{chen2022intra,
    title = {Intra-Instance VICReg: Bag of Self-Supervised Image Patch Embedding},
    author = {Yubei Chen and Adrien Bardes and Zengyi Li and Yann LeCun},
    journal={arXiv preprint arXiv:2206.08954}
    year = {2022},
    }</tt></pre>
            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract4">abstract</button>
            <div id="abstract4" class="collapse">
              <p style="text-align: justify;">
                Recently, self-supervised learning (SSL) has achieved tremendous empirical advancements in
                learning
                image representation. However, our understanding and knowledge of the representation are still
                limited.
                This work shows that the success of the SOTA siamese-network-based SSL approaches is primarily
                based
                on
                learning a representation of image patches. Particularly, we show that when we learn a
                representation
                only for fixed-scale image patches and aggregate different patch representations linearly for an
                image
                (instance), it can achieve on par or even better results than the baseline methods on several
                benchmarks. Further, we show that the patch representation aggregation can also improve various
                SOTA
                baseline methods by a large margin. We also establish a formal connection between the SSL
                objective
                and
                the image patches co-occurrence statistics modeling, which supplements the prevailing invariance
                perspective. By visualizing the nearest neighbors of different image patches in the embedding
                space
                and
                projection space, we show that while the projection has more invariance, the embedding space
                tends
                to
                preserve more equivariance and locality. Finally, we propose a hypothesis for the future
                direction
                based
                on the discovery of this work.
              </p>
            </div>
            <!-- <a href="https://github.com/facebookresearch/VICRegL"><button type="button"
                class="btn btn-primary btn-xs">code</button></a> -->
          </div>
        </div>
        <!-- end of Intra-Instance -->

        <!-- Duality -->
        <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="./img/duality.png" width="1738" height="763"
                alt="On the duality between contrastive and non-contrastive self-supervised learning">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>On the duality between contrastive and non-contrastive self-supervised learning
            </strong> </br>
            Quentin Garrido, Yubei Chen, <u>Adrien Bardes</u>, Laurent Najman and Yann Lecun<br>ICLR 2023 (Best
            paper
            award honorable mention)<br>
            <a href="https://arxiv.org/abs/2206.02574"><button type="button"
                class="btn btn-primary btn-xs">arxiv</button></a>
            <a href="https://arxiv.org/pdf/2206.02574.pdf"><button type="button"
                class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex3">bibtex</button>
            <div id="bibtex3" class="collapse">
              <pre><tt>@inproceedings{garrido2023duality,
    title = {On the duality between contrastive and non-contrastive self-supervised learning},
    author = {Quentin Garrido and Yubei Chen and Adrien Bardes and Laurent Najman and Yann Lecun},
    article={ICLR}
    year = {2023},
    }</tt></pre>
            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract3">abstract</button>
            <div id="abstract3" class="collapse">
              <p style="text-align: justify;">
                Recent approaches in self-supervised learning of image representations can be categorized into
                different
                families of methods and, in particular, can be divided into contrastive and non-contrastive
                approaches.
                While differences between the two families have been thoroughly discussed to motivate new
                approaches, we
                focus more on the theoretical similarities between them. By designing contrastive and covariance
                based
                non-contrastive criteria that can be related algebraically and shown to be equivalent under
                limited
                assumptions, we show how close those families can be. We further study popular methods and
                introduce
                variations of them, allowing us to relate this theoretical result to current practices and show
                the
                influence (or lack thereof) of design choices on downstream performance. Motivated by our
                equivalence
                result, we investigate the low performance of SimCLR and show how it can match VICReg's with
                careful
                hyperparameter tuning, improving significantly over known baselines. We also challenge the
                popular
                assumptions that contrastive and non-contrastive methods, respectively, need large batch sizes
                and
                output dimensions. Our theoretical and quantitative results suggest that the numerical gaps
                between
                contrastive and non-contrastive methods in certain regimes can be closed given better network
                design
                choices and hyperparameter tuning. The evidence shows that unifying different SOTA methods is an
                important direction to build a better understanding of self-supervised learning.
              </p>
            </div>
          </div>
        </div>
        <!-- end of Duality -->

        <!-- VICRegL, NeurIPS 2022 -->
        <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="./img/vicregl.jpg" width="1738" height="763"
                alt="VICRegL: Self-Supervised Learning of Local Visual Features">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>VICRegL: Self-Supervised Learning of Local Visual Features</strong> </br>
            <u>Adrien Bardes</u>, Jean Ponce, and Yann LeCun<br>
            NeurIPS 2022 <br>
            <a target="_blank" href="https://arxiv.org/abs/2210.01571"> <button type="button"
                class="btn btn-primary btn-xs"> arxiv </button></a>
            <a target="_blank" href="https://arxiv.org/pdf/2210.01571.pdf"><button type="button"
                class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex6">bibtex</button>
            <div id="bibtex6" class="collapse">
              <pre><tt>@inproceedings{bardes2022vicregl,
            title = {VICRegL: Self-Supervised Learning of Local Visual Features},
            author = {Adrien Bardes and Jean Ponce and Yann LeCun},
            booktitle={arXiv preprint arXiv:2206.08155}
            year = {2022},
            }</tt></pre>

            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract6">abstract</button>
            <div id="abstract6" class="collapse">
              <p style="text-align: justify;">
                Most recent self-supervised methods for learning image representations focus on
                either producing a global feature with invariance properties, or producing a set of
                local features. The former works best for classification tasks while the latter is best
                for detection and segmentation tasks. This paper explores the fundamental trade-
                off between learning local and global features. A new method called VICRegL
                is proposed that learns good global and local features simultaneously, yielding
                excellent performance on detection and segmentation tasks while maintaining
                good performance on classification tasks. Concretely, two identical branches of a
                standard convolutional net architecture are fed two differently distorted versions
                of the same image. The VICReg criterion is applied to pairs of global feature
                vectors. Simultaneously, the VICReg criterion is applied to pairs of local feature
                vectors occurring before the last pooling layer. Two local feature vectors are
                attracted to each other if their l2-distance is below a threshold or if their relative
                locations are consistent with a known geometric transformation between the two
                input images. We demonstrate strong performance on linear classification and
                segmentation transfer tasks. Code and pretrained models are publicly available at:
                https://github.com/facebookresearch/VICRegL
              </p>
            </div>
            <a target="_blank" href="https://github.com/facebookresearch/VICRegL"><button type="button"
                class="btn btn-primary btn-xs">code</button></a>
          </div>
        </div>
        <!-- end of VICRegL -->


        <!-- VICReg, ICLR 2022 -->
        <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="./img/vicreg.jpg" width="1738" height="763"
                alt="VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning
            </strong> </br>
            <u>Adrien Bardes</u>, Jean Ponce and Yann Lecun<br>
            ICLR 2022 <br>
            <a href="https://arxiv.org/abs/2105.04906"><button type="button"
                class="btn btn-primary btn-xs">arxiv</button></a>
            <a href="https://arxiv.org/pdf/2105.04906.pdf"><button type="button"
                class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex2">bibtex</button>
            <div id="bibtex2" class="collapse">
              <pre><tt>@inproceedings{bardes2022vicreg,
    title = {VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning},
    author = {Adrien Bardes and Jean Ponce and Yann Lecun},
    booktitle = {ICLR}
    year = {2022},
    }</tt></pre>
            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract2">abstract</button>
            <div id="abstract2" class="collapse">
              <p style="text-align: justify;">
                Recent self-supervised methods for image representation learning are based on maximizing the
                agreement
                between embedding vectors from different views of the same image. A trivial solution is obtained
                when
                the encoder outputs constant vectors. This collapse problem is often avoided through implicit
                biases
                in
                the learning architecture, that often lack a clear justification or interpretation. In this
                paper,
                we
                introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly
                avoids
                the
                collapse problem with a simple regularization term on the variance of the embeddings along each
                dimension individually. VICReg combines the variance term with a decorrelation mechanism based
                on
                redundancy reduction and covariance regularization, and achieves results on par with the state
                of
                the
                art on several downstream tasks. In addition, we show that incorporating our new variance term
                into
                other methods helps stabilize the training and leads to performance improvements.
              </p>
            </div>
            <a href="https://github.com/facebookresearch/vicreg"><button type="button"
                class="btn btn-primary btn-xs">code</button></a>
          </div>
        </div>
        <!-- end of VICReg -->


        <!-- Hallucinates, ICCV 2022 -->
        <div class="row">
          <div class="col-xs-10 col-sm-4 col-md-4">
            <a class="thumbnail">
              <img src="./img/dmas.png" width="1738" height="763"
                alt="Learning To Hallucinate Examples From Extrinsic and Intrinsic Supervision">
            </a>
          </div>
          <div class="col-xs-12 col-sm-8 col-md-8">
            <strong>Learning To Hallucinate Examples From Extrinsic and Intrinsic Supervision
            </strong> </br>
            Liangke Gui*, <u>Adrien Bardes</u>*, Ruslan Salakhutdinov, Alexander Hauptmann, Martial Hebert and
            Yu-Xiong
            Wang<br>
            ICCV 2022 <br>
            <!-- <a href="https://arxiv.org/abs/2105.04906"><button type="button"
                class="btn btn-primary btn-xs">arxiv</button></a> -->
            <a
              href="https://openaccess.thecvf.com/content/ICCV2021/papers/Gui_Learning_To_Hallucinate_Examples_From_Extrinsic_and_Intrinsic_Supervision_ICCV_2021_paper.pdf"><button
                type="button" class="btn btn-primary btn-xs">pdf</button></a>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#bibtex1">bibtex</button>
            <div id="bibtex1" class="collapse">
              <pre><tt>@inproceedings{gui2021dmas,
    title = {Learning To Hallucinate Examples From Extrinsic and Intrinsic Supervision},
    author = {Liangke Gui and Adrien Bardes and Ruslan Salakhutdinov and Alexander Hauptmann and Martial Hebert and Yu-Xiong},
    booktitle = {ICCV 2021}
    year = {2021},
    }</tt></pre>
            </div>
            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
              data-target="#abstract1">abstract</button>
            <div id="abstract1" class="collapse">
              <p style="text-align: justify;">
                Learning to hallucinate additional examples has recently been shown as a promising direction to
                address
                few-shot learning tasks. This work investigates two important yet overlooked natural supervision
                signals
                for guiding the hallucination process--(i) extrinsic: classifiers trained on hallucinated
                examples
                should be close to strong classifiers that would be learned from a large amount of real
                examples;
                and
                (ii) intrinsic: clusters of hallucinated and real examples belonging to the same class should be
                pulled
                together, while simultaneously pushing apart clusters of hallucinated and real examples from
                different
                classes. We achieve (i) by introducing an additional mentor model on data-abundant base classes
                for
                directing the hallucinator, and achieve (ii) by performing contrastive learning between
                hallucinated
                and
                real examples. As a general, model-agnostic framework, our dual mentor-and self-directed (DMAS)
                hallucinator significantly improves few-shot learning performance on widely used benchmarks in
                various
                scenarios.
              </p>
            </div>
            <!-- <a href="https://github.com/facebookresearch/vicreg"><button type="button"
                class="btn btn-primary btn-xs">code</button></a> -->
          </div>
        </div>
        <!-- end of Hallucinate -->

      </div>
    </div> <!-- end of projects -->

    <hr>

    <!-- Talks -->
    <div class="row" id="talks" style="padding-top:30px; margin-top:-60px;">
      <div class="col-md-12">
        <h2>Talks</h2>
        <ul>
          <!-- <li>06/2022 - <a href="https://cvpr2022.thecvf.com/">CVPR 2022 Oral Session (New Orleans, Louisiana) </a> - <a
              href="https://www.youtube.com/watch?v=FT_50ylQMNs&ab_channel=AntoineYang">[5 min Video] </a> - <a
              href="slides/tubedetr-cvpr.pdf"> [Slides] </a> - <a href="slides/tubedetr-cvpr-poster.pdf"> [Poster] </a>
          </li>
          <li>06/2022 - <a href="https://www.di.ens.fr/"> Seminar of the Computer Science department of &Eacute;cole
              Normale Sup&eacute;rieure (Mûr-de-Bretagne, France) </a> - <a href="slides/frozenbilm-diens.pdf"> [Slides]
            </a> </li>
          <li>10/2021 - <a href="https://iccv2021.thecvf.com/home">ICCV 2021 Oral Session (Virtual) </a> - <a
              href="https://www.youtube.com/watch?v=eyEW5Z23Ofs&ab_channel=AntoineYang">[12 min Video] </a> - <a
              href="slides/just-ask-iccv.pdf"> [Slides] </a> - <a href="slides/just-ask-iccv-poster.pdf"> [Poster] </a>
          </li>
          <li>06/2021 - <a href="https://holistic-video-understanding.github.io/workshops/cvpr2021.html">CVPR 2021
              Holistic Video Understanding Workshop (Virtual) </a> - <a href="slides/just-ask-hvu-cvpr21.pdf"> [Slides]
            </a> - <a href="https://youtu.be/jzXdRT5W3C4?t=17280"> [Recording] </a> </li>
          <li>05/2021 - <a href="https://www.rocq.inria.fr/semdoc/">Inria Junior Seminar (Virtual) </a> - <a
              href="https://www.rocq.inria.fr/semdoc/Presentations/just-ask-junior-seminar.pdf"> [Slides] </a> - <a
              href="https://webconf.math.cnrs.fr/playback/presentation/2.0/playback.html?meetingId=38b91f46f61e82195e2275670f5d26218e2f9bae-1621340178022">
              [Recording] </a> </li>
          <li>04/2020 - <a href="https://iclr.cc/virtual_2020/poster_HygrdpVKvr.html">ICLR 2020 Poster Session (Virtual)
            </a> - <a href="https://www.youtube.com/watch?v=jB7vrGR_4ak&ab_channel=AntoineYang">[5 min Video] </a> - <a
              href="slides/nasefh-iclr20.pdf"> [Slides] </a></li> -->

          <!-- <li>06/2022 - <a href="https://cvpr2022.thecvf.com/">CVPR 2022 Oral Session (New Orleans, Louisiana) </a> - <a
              href="https://www.youtube.com/watch?v=FT_50ylQMNs&ab_channel=AntoineYang">[5 min Video] </a> - <a
              href="slides/tubedetr-cvpr.pdf"> [Slides] </a> - <a href="slides/tubedetr-cvpr-poster.pdf"> [Poster] </a>
          </li> -->

          <li>Self-supervised learning, theory and applications, Swood Partners, 2023</li>
          <li>Self-supervised learning of local visual features, Mila Computer Vision Meeting, 2022</li>
          <li>Self-supervised learning and thrust-worthy AI, Confiance.AI, 2022</li>
          <li>Self-supervised learning of local visual features, NeurIPS poster session, 2022</li>
          <li>Variance-Invariance-Covariance for Self-Supervised Learning, ICLR poster session, 2022</li>
          <li>Variance-Invariance-Covariance for Self-Supervised Learning, FAIR Workshop, 2021</li>
        </ul>
      </div>
    </div>
    <!-- <hr> -->

    <!-- Teaching -->
    <div class="row" id="teaching" style="padding-top:30px; margin-top:-60px;">
      <div class="col-md-12">
        <h2>Teaching</h2>
        <ul>
          <li>Object Recognition and Computer Vision, Project Advisor, Master level (MVA), ENS Paris-Saclay,
            Fall
            2021
          </li>
          <li>Object Recognition and Computer Vision, Project Advisor, Master level (MVA), ENS Paris-Saclay,
            Fall
            2022
          </li>
        </ul>

      </div>
    </div>
    <!-- end of teaching -->

    <!-- <hr> -->

    <!-- Misc -->
    <div class="row" id="misc" style="padding-top:30px; margin-top:-60px;">
      <div class="col-md-12">
        <h2>Academic Services</h2>
        Reviewer for IJVC, ML (Springer), ICLR 2022-2023, NeurIPS 2022-2023, CVPR 2023, ICCV 2023.
      </div>
    </div> <!-- end of misc -->

    <hr>

    <div class="container">
      <footer>
        <p align="right"><small>Copyright © Adrien Bardes &nbsp;/&nbsp; Last update: July 2023 </small></p>
      </footer>
      <div style="height:10px;"></div>
    </div>

    <!-- Bootstrap core JavaScript -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="jq/jquery-1.11.1.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/docs.min.js"></script>

    <script>
      (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
          (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
          m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
      })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

      ga('create', 'UA-75922772-1', 'auto');
      ga('send', 'pageview');

    </script>
  </div>
</body>

</html>